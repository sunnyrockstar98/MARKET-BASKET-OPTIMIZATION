{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In general whenever we go to buy bread to a store we end up buying milk or butter/cheese as well.\n",
    "\n",
    "And if you go to buy these groceries from a supermarket chances are you might have encountered this weird thing\n",
    "\n",
    "    Bread and butter are kept in totally opposite corners of the supermarket. \n",
    "\n",
    "*Well this is not by accident.*\n",
    "\n",
    "\n",
    "\n",
    "This is to make you go through the whole store for your butter/cheese so that you go on by looking at various products on the way and may end up buying something you had not earlier planned for.\n",
    "\n",
    "And there can be innumerable strategies like someone who went to buy cereals ended up buying milk as well just because it was immediately on the next rack to cereals. Finding Association is the key. Then all that remains is to build startegies to maximize efficiency and revenue generation.\n",
    "\n",
    "Market Basket Analysis is one of the most important stepping stone to understand how Netflix or Amazon build such world class recommender systems.\n",
    "\n",
    "We here work on finding creative and out-of-the-box rules. Rules are nothing but these associations between various products some imaginable and some totally out of the blue.\n",
    "\n",
    "Maybe somewhere you could end up finding that people who buy tomato sauce more also tend to buy comic books more. Although there seems no reasonable explanation right now but after good statistical analysis some relation might be found.\n",
    "\n",
    "This is what Association Rule Learning wishes to achieve. To give you insights and relations which you otherwise would have never thought off!!\n",
    "\n",
    "Now of course these rules need the **support** of huge (in hundreds at least) number of transactions to be considered statistically signifcant.\n",
    "\n",
    "Thus more data you have access to the better!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Why should I care about these associations?\n",
    "\n",
    "Supermarkets these days actually use Data Science to strategically set their store to increase their sales.\n",
    "The Association of Bread and Butter is pretty obvious and simple but there are 1000s of products in a store and you never know which pair of products may have high association.\n",
    "\n",
    "For the sake of simplicity assume there are 2 products P and Q.\n",
    "\n",
    "If we can conclude that P and Q have strong association we can do the following:\n",
    "\n",
    "*  Put both P and Q on the same shelf so that buyers of one item would be prompted to buy the other.\n",
    "*  Target advertisments for P to customers who buy Q more often.\n",
    "*  Combine P and Q somehow as 1 product\n",
    "*  Apply buy 1 get 1 offers on P and Q for promotions.\n",
    "\n",
    "\n",
    "### So have you ever wondered how Amazon shows \"Customers who bought this also bought\"??\n",
    "\n",
    "\n",
    "![amazon-suggestion.png](amazon-suggestion.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well this is how..\n",
    "\n",
    "There is actually an algorithm called **Apriori** to find associations like these. Once you find these associations you can strategize on how to use these associations cleverly to set up your stores.\n",
    "\n",
    "This is a plus-plus for any business.\n",
    "\n",
    "Now this Apriori is a basic model. Amazon actually uses a complex set of algorithms along with this to maximize their revenues.\n",
    "\n",
    "\n",
    "We will try implementing an Apriori model ourselves but before that have a look at what the algorithm is:-"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Association Rule Learning and Apriori algorithm.\n",
    "\n",
    "We need to learn a bit of jargon to proceed further..\n",
    "\n",
    "\n",
    "**Support** is a relative frequency that the rules show. This basically says how popular an item is in the transactions dataset i.e. in the big transactions dataset how frequently the particular item came up.\n",
    "\n",
    "In many cases in order to make sure there is high relationship we might want to use high support.\n",
    "However, there may be cases where a low support is beneficial - in case of **“unusual”** relationships.\n",
    "\n",
    "**Confidence** is a measure of the reliability of the rule.It simply asks how likely item Q is purchased when item P is already purchased. \n",
    "\n",
    "One issue with Confidence is that it can misrepresent the importance of some association. You need to be carefull with this parameter. I will explain it when we do parameter selection.\n",
    "\n",
    "**Lift** is the ratio of the support(**observed**) to that expected if the two rules were independent. In layman terms it tells how likely item Q is purchased when item P is purchased, while controlling how popular item Q is.\n",
    "\n",
    "The apriori algorithm simply states that if an item does not occur frequently, then all its subsets must also not occur frequently. \n",
    "\n",
    "Here subsets are collection of items which ever were together in any transaction.\n",
    "\n",
    "![formula.png](formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The algorithm is:**\n",
    "\n",
    "1. Set a minimum value for support and confidence\n",
    "\n",
    "2. Take all subsets in transactions having support higher than minimum support we chose.\n",
    "\n",
    "3. Now take all rules having higher confidence than confidence we chose\n",
    "\n",
    "4. Sort the rules by decreasing lift.\n",
    "\n",
    "#### It will get pretty clear once we start coding!\n",
    "\n",
    "Let's start with importing whatever we would need to build these associations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by importing the basic libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The customer cart(basket) data which we need is taken from a course Machine Learning A-Z from Udemy, taught by Kirilenko.\n",
    "\n",
    "We have to use **\"header = None\"** while reading the dataset so that the first row doesn't become the column headers.\n",
    "Let's load the dataset using pandas in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what kind of data we are dealing with using head for dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>almonds</td>\n",
       "      <td>avocado</td>\n",
       "      <td>vegetables mix</td>\n",
       "      <td>green grapes</td>\n",
       "      <td>whole weat flour</td>\n",
       "      <td>yams</td>\n",
       "      <td>cottage cheese</td>\n",
       "      <td>energy drink</td>\n",
       "      <td>tomato juice</td>\n",
       "      <td>low fat yogurt</td>\n",
       "      <td>green tea</td>\n",
       "      <td>honey</td>\n",
       "      <td>salad</td>\n",
       "      <td>mineral water</td>\n",
       "      <td>salmon</td>\n",
       "      <td>antioxydant juice</td>\n",
       "      <td>frozen smoothie</td>\n",
       "      <td>spinach</td>\n",
       "      <td>olive oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>meatballs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chutney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkey</td>\n",
       "      <td>avocado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mineral water</td>\n",
       "      <td>milk</td>\n",
       "      <td>energy bar</td>\n",
       "      <td>whole wheat rice</td>\n",
       "      <td>green tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1           2                 3             4   \\\n",
       "0         shrimp    almonds     avocado    vegetables mix  green grapes   \n",
       "1        burgers  meatballs        eggs               NaN           NaN   \n",
       "2        chutney        NaN         NaN               NaN           NaN   \n",
       "3         turkey    avocado         NaN               NaN           NaN   \n",
       "4  mineral water       milk  energy bar  whole wheat rice     green tea   \n",
       "\n",
       "                 5     6               7             8             9   \\\n",
       "0  whole weat flour  yams  cottage cheese  energy drink  tomato juice   \n",
       "1               NaN   NaN             NaN           NaN           NaN   \n",
       "2               NaN   NaN             NaN           NaN           NaN   \n",
       "3               NaN   NaN             NaN           NaN           NaN   \n",
       "4               NaN   NaN             NaN           NaN           NaN   \n",
       "\n",
       "               10         11     12     13             14      15  \\\n",
       "0  low fat yogurt  green tea  honey  salad  mineral water  salmon   \n",
       "1             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "2             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "3             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "4             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "\n",
       "                  16               17       18         19  \n",
       "0  antioxydant juice  frozen smoothie  spinach  olive oil  \n",
       "1                NaN              NaN      NaN        NaN  \n",
       "2                NaN              NaN      NaN        NaN  \n",
       "3                NaN              NaN      NaN        NaN  \n",
       "4                NaN              NaN      NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description about the dataset\n",
    "\n",
    "The dataset is a collection of final checkout carts (1 week) of the last 7500 customers from a store. \n",
    "\n",
    "- Each row represents the items taken out by a customer. For example 1 represents that 2nd customer taking out **burgers, meatballs and eggs** from the store.\n",
    "\n",
    "- The colums have no significance as there is no limit to how many items the customer can buy.\n",
    "\n",
    "- NaN values are just to fill up the empty cells. For e.g. 2nd customer only bought 3 items while 1st customer bought 19.To have the whole dataset in a tabular form NaN values are introduced \n",
    "\n",
    "**Basically each row is 1 complete customer transaction at the checkout counter.** \n",
    "\n",
    "Everytime any customer comes for checkout and bill payment his/her items are stored in the database row-wise.\n",
    "\n",
    "Now to run our apriori algorithm successfully we would require a library called **apyori.py** (an implementation of the Association Rule Learning model) from the **Python Software Foundation.**\n",
    "\n",
    "You can download the file from here\n",
    "\n",
    "https://github.com/ymoch/apyori/blob/master/apyori.py\n",
    "\n",
    "Make sure you save **this file** along with **the dataset** and **this script** in the same folder for ease later.\n",
    "\n",
    "So we need to give this dataset as an input to this apyori model. But this model expects the input as a list of lists.\n",
    "\n",
    "So we need to do some data pre-processing:(convert dataset into transactions a list of lists.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "transactionset = []\n",
    "for i in range(0, 7501):\n",
    "    transactionset.append([str(dataset.values[i,j]) for j in range(0, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support - Confidence - Lift\n",
    "\n",
    "Next step is to decide minimum support, minimum confidence and minimum lift for the model. These are parameters which determine association or more specifically assume there are 2 products A and B. If in a population someone likes A what is the chance that the same person will also like B. Using minimum parameters help set required threshold for considering wether a particular association is legit or not.\n",
    "\n",
    "These parameters vary from problem to problem. They depend on:\n",
    "\n",
    "1. Nature of the dataset\n",
    "2. Problem Description\n",
    "3. Number of entries in a dataset.\n",
    "\n",
    "So we train the apriori model by giving paramters and transactions as input and obtain a list of rules as an output for our problem.\n",
    "\n",
    "You can try these rules for a certain period of time and if you don't find an impact on your revenue you may change parameters to test new rules and experiment till you find the strongest rules to maximize your revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Apriori on the dataset\n",
    "from apyori import apriori\n",
    "rules = apriori(transactionset, min_support = 0.004, min_confidence = 0.2, min_lift = 3, min_length = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to chose the parameters??\n",
    "\n",
    "If you have gone through the algorithm description above you clearly now know what support,lift,etc. are.\n",
    "\n",
    "These values for parameters depend on your business goals and if you are not satisfied with your rules you can change them again!\n",
    "\n",
    "Now for sure in this case we would want to find strong rules about items that are bought at least 3 or 4 times in a day and associating those items together would eventually increase sales.\n",
    "\n",
    "Now the 7500 transactions are recorded over a week so if we find a product purchased 4 times daily it means 4*7 = 28 times a week.\n",
    "\n",
    "Hence **min_support** = 28/7500 = 0.004;\n",
    "\n",
    "Now **min_confidence** is the tricky one. If I keep **min_confidence = 0.9 or 0.8** it means my rules have to be correct **90% of the time which is not a good indicator of association** because it means in a group each product is by itself a frequently purchased one and not associated with other.\n",
    "\n",
    "So again hit-and-trial and see what works with your dataset. For these 7500 transactions a **20% confidence seemed to work nice.**\n",
    "\n",
    "80% is default generally. Ideally you should keep dividing by 2 untill you get satisfying rules. **min_lift** is set to 3 to give us relevant rules only.\n",
    "\n",
    "**min_length** is set to minimum number of associations you want in your rules.We want at least 2 associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "results = list(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will now get printed in decreasing order of relevance along with support for that particular association. \n",
    "\n",
    "The relevance is decided automatically by the apriori library and is based on support,confidence as well as lift.\n",
    "\n",
    "We print here 1/4 th of the association results to have a look at the best ones only.\n",
    "\n",
    "**RULE**  --> means which two products are associated.\n",
    "\n",
    "**SUPPORT**  --> popularity of that rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RULE:\t['light cream', 'chicken']\n",
      "SUPPORT:\t0.004532728969470737\n",
      "RULE:\t['mushroom cream sauce', 'escalope']\n",
      "SUPPORT:\t0.005732568990801226\n",
      "RULE:\t['pasta', 'escalope']\n",
      "SUPPORT:\t0.005865884548726837\n",
      "RULE:\t['herb & pepper', 'ground beef']\n",
      "SUPPORT:\t0.015997866951073192\n",
      "RULE:\t['ground beef', 'tomato sauce']\n",
      "SUPPORT:\t0.005332622317024397\n",
      "RULE:\t['olive oil', 'whole wheat pasta']\n",
      "SUPPORT:\t0.007998933475536596\n",
      "RULE:\t['pasta', 'shrimp']\n",
      "SUPPORT:\t0.005065991201173177\n",
      "RULE:\t['nan', 'light cream', 'chicken']\n",
      "SUPPORT:\t0.004532728969470737\n",
      "RULE:\t['chocolate', 'frozen vegetables', 'shrimp']\n",
      "SUPPORT:\t0.005332622317024397\n",
      "RULE:\t['spaghetti', 'cooking oil', 'ground beef']\n",
      "SUPPORT:\t0.004799360085321957\n",
      "RULE:\t['herb & pepper', 'ground beef', 'eggs']\n",
      "SUPPORT:\t0.0041327822956939075\n",
      "RULE:\t['mushroom cream sauce', 'escalope', 'nan']\n",
      "SUPPORT:\t0.005732568990801226\n",
      "RULE:\t['pasta', 'escalope', 'nan']\n",
      "SUPPORT:\t0.005865884548726837\n",
      "RULE:\t['frozen vegetables', 'spaghetti', 'ground beef']\n",
      "SUPPORT:\t0.008665511265164644\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, int(len(results)/4)):\n",
    "    print('RULE:\\t' + str(list(results[i][0])) + '\\nSUPPORT:\\t' + str(results[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So like the highest association is found between chicken and light curry with a support of 0.0045 which kind of makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In short we have done simply the following:\n",
    "\n",
    "1. Used a dataset with 7500 customer transaction history of 1 week\n",
    "2. Using the dataset built a program to find associations between different products\n",
    "3. Made sure these associations are relevant.\n",
    "4. Sorted the association rules by relevance.\n",
    "\n",
    "\n",
    "I hope you understood this basic tutorial on how to find key associations which would generally beat common sense and how you can use these rules that you have found to build strategies and later take decisions for your business and maximize sales and efficiency.\n",
    "\n",
    "Do try the code yourself and experiment on other datasets as well and figure how can improve your associations.\n",
    "\n",
    "Some links where you can find similiar datsets:\n",
    "\n",
    "1. https://www.kaggle.com/c/instacart-market-basket-analysis/data  \n",
    "2. http://fimi.ua.ac.be/data/ \n",
    "3. http://archive.ics.uci.edu/ml/datasets.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can connect with me on LinkedIn throught this - https://www.linkedin.com/in/rishabh-baid-8b04a6133/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
